#!/usr/bin/env python3
"""
main.py – Patch a DCR‑creation Bicep file and (optionally) emit a PowerShell
script that creates the corresponding custom table.

Usage
-----
python3 main.py <bicep-file> --table-name <CustomTableName_CL>

If the Bicep file does **not** contain a literal `columns: [` block, the script
still emits *<bicep-file>* and only skips generation of the companion
`*_createAuxTable.ps1` helper.

**Change log**
2025‑06‑04  ●  `schema_block()` now *filters out* the `TimeGenerated` column so
             we don’t accidentally duplicate it (the PS script adds it on its
             own).
"""
from __future__ import annotations
import argparse, pathlib, re, textwrap
from string import Template

API_VERSION = "2023-01-01-preview"
# ────────────────────────────────────────────────────────────────────────────
# PowerShell template – uses Template‑style placeholders (${schema}, etc.)
PS_CREATE = Template(textwrap.dedent(r"""
<#
  Auto-generated by $src
#>

param(
    [string]$ResourceGroup   = 'rg-sentinel-cloud-fw',
    [string]$WorkspaceName   = 'log-sentinel-cloud-fw',
    [string]$TableName       = '$table',   # must end in _CL
    [int]   $RetentionInDays = 365,
    [string]$SubscriptionId  = (az account show --query id -o tsv)
)

$schemaText = @'
    ${schema}
'@

$columns = @(@{ name = 'TimeGenerated'; type = 'datetime' })
$schemaText.Trim().Split("`n") | ForEach-Object {
    if (-not $_.Trim()) { return }
    $name,$type = $_.Trim() -split ':',2
    $columns += @{ name = $name.Trim(); type = $type.Trim().ToLower() }
}

$uri = "https://management.azure.com" +
       "/subscriptions/$SubscriptionId" +
       "/resourceGroups/$ResourceGroup" +
       "/providers/Microsoft.OperationalInsights" +
       "/workspaces/$WorkspaceName/tables/$TableName" +
       "?api-version=${api_version}"

$body = @{ properties = @{ schema = @{ name = $TableName; columns = $columns }
                            totalRetentionInDays = $RetentionInDays
                            plan = 'Auxiliary' } } |
         ConvertTo-Json -Depth 5 -Compress

Write-Host "`nCreating '$TableName' …`n" -Foreground Cyan
az rest --method PUT --uri $uri --body $body
Write-Host "`n✓  Done." -Foreground Green
"""))
# ────────────────────────────────────────────────────────────────────────────
# Regex helpers for patching the Bicep file
STREAM_START    = re.compile(r"^\s*streams\s*=\s*\[\s*$", re.I)
DATAFLOWS_START = re.compile(r"^\s*dataFlows\s*:\s*\[\s*$", re.I)

def OUT_RE(tbl: str):
    return re.compile(rf"outputStream\s*:\s*['\"]Custom-{re.escape(tbl)}['\"]", re.I)

# ---------------------------------------------------------------------------
# Utility helpers

def ensure_trailing_comma(lines: list[str]) -> None:
    """Ensure the last non‑blank line ends with a comma so we can safely append."""
    for i in range(len(lines) - 1, -1, -1):
        if lines[i].strip():
            if not lines[i].rstrip().endswith(','):
                lines[i] += ','
            break

def patch_bicep(src: pathlib.Path, table: str, dest: pathlib.Path) -> None:
    """Inject the extra stream + dataFlow and pin api‑version."""
    lines = src.read_text("utf-8").splitlines()
    out, it = [], iter(enumerate(lines))
    inserted_stream = inserted_flow = False
    out_re = OUT_RE(table)

    for _, ln in it:
        out.append(ln)

        # ----- streams[] ---------------------------------------------------
        if STREAM_START.match(ln):
            indent = re.match(r"\s*", ln).group(0) + "  "
            buf, depth = [], 1
            for _, l in it:
                buf.append(l)
                depth += l.count("[") - l.count("]")
                if depth == 0:                           # closing ]
                    ensure_trailing_comma(buf)
                    if not any(table in s for s in buf):
                        buf.insert(-1, f"{indent}'{table}'")
                        inserted_stream = True
                    out.extend(buf)
                    break
            continue

        # ----- dataFlows[] -------------------------------------------------
        if DATAFLOWS_START.match(ln):
            df_indent = re.match(r"\s*", ln).group(0) + "  "
            buf, depth, found = [], 1, False
            for _, l in it:
                buf.append(l)
                depth += l.count("[") - l.count("]")
                if out_re.search(l):
                    found = True
                if depth == 0:                           # reached closing ]
                    if not found:
                        closing_bracket = buf.pop()     # remove ']'
                        ensure_trailing_comma(buf)
                        buf.append(f"{df_indent}{{")
                        buf.append(f"{df_indent}  streams: [")
                        buf.append(f"{df_indent}    'Custom-nss_fw_CL'")
                        buf.append(f"{df_indent}  ]")
                        buf.append(f"{df_indent}  destinations: [")
                        buf.append(f"{df_indent}    workspaceId")
                        buf.append(f"{df_indent}  ]")
                        buf.append(f"{df_indent}  outputStream: 'Custom-{table}'")
                        buf.append(f"{df_indent}}}")
                        buf.append(closing_bracket)      # put ']' back
                        inserted_flow = True
                    out.extend(buf)
                    break
            continue

    # Pin api‑version everywhere in the file
    patched = re.sub(r"\?api-version=[^'\"\s]+",
                     f"?api-version={API_VERSION}",
                     "\n".join(out) + "\n",
                     flags=re.I)
    dest.write_text(patched, "utf-8")
    print(f"✓ wrote {dest.name} (streams {'added' if inserted_stream else 'ok'}, "
          f"dataFlow {'added' if inserted_flow else 'ok'}, api-version → {API_VERSION})")

# ---------------------------------------------------------------------------
# Helper: extract column list from the Bicep (lenient)
COL_BLOCK_RE = re.compile(r"columns\s*:\s*\[(.*?)\]", re.S | re.I)
NAME_RE      = re.compile(r"name\s*:\s*'([^']+)'", re.I)
TYPE_RE      = re.compile(r"type\s*:\s*'([^']+)'", re.I)

def columns_from(txt: str) -> list[tuple[str, str]]:
    m = COL_BLOCK_RE.search(txt)
    if not m:
        return []
    block = m.group(1)
    cols: list[tuple[str, str]] = []
    for obj in re.finditer(r"{(.*?)}", block, re.S):
        snippet = obj.group(1)
        n = NAME_RE.search(snippet)
        t = TYPE_RE.search(snippet)
        if n and t:
            cols.append((n.group(1), t.group(1)))
    return cols

# Format schema for PS script, skipping TimeGenerated (case‑insensitive)

def schema_block(cols: list[tuple[str, str]]) -> str:
    return "\n".join(f"{c}:{t}" for c, t in cols if c.lower() != "timegenerated")

# ---------------------------------------------------------------------------
# Entry‑point

def main() -> None:
    ap = argparse.ArgumentParser(description="Patch DCR Bicep and create helper PS script.")
    ap.add_argument("bicep_file", type=pathlib.Path)
    ap.add_argument("--table-name", required=True, help="Custom table name, must end in _CL")
    args = ap.parse_args()

    src  = args.bicep_file
    tbl  = args.table_name

    text = src.read_text("utf-8")
    cols = columns_from(text)

    if cols:
        ps1_content = PS_CREATE.safe_substitute(src=src.name,
                                                table=tbl,
                                                schema=schema_block(cols),
                                                api_version=API_VERSION)
        ps1_path = src.with_name(src.stem + "_createAuxTable.ps1")
        ps1_path.write_text(ps1_content, "utf-8")
        print(f"✓ wrote {ps1_path.name} (columns={len(cols)})")
    else:
        print("⚠️  No column block found – skipping *_createAuxTable.ps1 generation.")

    dest = src.with_name(f"{src.name}")
    patch_bicep(src, tbl, dest)

if __name__ == "__main__":
    main()
